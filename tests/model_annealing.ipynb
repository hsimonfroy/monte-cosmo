{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model annealing\n",
    "Infer a cosmological model via Continuous Tempering Langevin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feynmangpu04.cluster.local\n",
      "gpu [cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!python -c \"import jax; print(jax.default_backend(), jax.devices())\"\n",
    "# !nvidia-smi\n",
    "# numpyro.set_platform(\"gpu\")\n",
    "\n",
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='.33' # NOTE: jax preallocates GPU (default 75%)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, vmap, grad\n",
    "\n",
    "import numpyro\n",
    "from numpyro.handlers import seed, condition, trace\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"Continuous Tempering Langevin\");\n",
    "# mlflow.end_run()\n",
    "# mlflow.start_run(run_name=\"Zee\")\n",
    "# mlflow.log_params({\"ho\":2, \"ha\":np.array([2,3])})\n",
    "# mlflow.log_metrics({\"ho\":2, \"ha\":3}, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run\n",
    "mlflow.start_run()\n",
    "run = mlflow.last_active_run()\n",
    "run.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and simulate fiducial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 16:08:17.832845: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 11.5 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config={'mesh_size': array([64, 64, 64]), 'box_size': array([640, 640, 640]), 'scale_factor_lpt': 0.5, 'scale_factor_obs': 0.5, 'galaxy_density': 0.001, 'trace_reparam': True, 'trace_deterministic': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/feynman/work/dphp/hs276503/miniforge3/envs/montecosmoenv/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:66: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype)\n"
     ]
    }
   ],
   "source": [
    "from montecosmo.models import pmrsd_model, model_config\n",
    "from montecosmo.utils import get_simulator, get_logp_fn, get_score_fn\n",
    "model_config['scale_factor_lpt'] = 0.5\n",
    "model_config['scale_factor_obs'] = 0.5\n",
    "print(f\"{model_config=}\")\n",
    "\n",
    "model = partial(pmrsd_model, **model_config)\n",
    "\n",
    "# Cosmological parameters\n",
    "cosmo_names = ['Omega_c', 'sigma8']\n",
    "cosmo_labels = [r'\\Omega_c', r'\\sigma_8']\n",
    "cond_params = {var_name+'_base': 0. for var_name in cosmo_names}\n",
    "\n",
    "fiducial_simulator = get_simulator(model, cond_params)\n",
    "fiducial_params = fiducial_simulator(batch_size=1)\n",
    "fiducial_cosmo_params = {name: fiducial_params[name] for name in cosmo_names}\n",
    "\n",
    "# Condition model\n",
    "obs_names = ['obs_mesh'] # NOTE: Only condition on random sites\n",
    "obs_params = {name: fiducial_params[name][0] for name in obs_names}\n",
    "observed_model = condition(model, obs_params)\n",
    "logp_fn = get_logp_fn(observed_model)\n",
    "score_fn = get_score_fn(observed_model)\n",
    "\n",
    "# Parameters to initialize samplers on\n",
    "init_names = ['Omega_c', 'sigma8', 'init_mesh', 'b1', 'b2', 'bs', 'bnl'] # NOTE: Only init on random sites\n",
    "init_params = {name+'_base': fiducial_params[name+'_base'] for name in init_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpyro.infer.util import log_density\n",
    "\n",
    "# params = {name: fiducial_params[name][0] for name in fiducial_params.keys()}\n",
    "# logp_unvec = log_density(model=observed_model, \n",
    "#                             model_args=(), \n",
    "#                             model_kwargs={}, \n",
    "#                             params=params)[0]\n",
    "\n",
    "\n",
    "# logp_vec = logp_fn(fiducial_params)\n",
    "# score_vec = score_fn(fiducial_params)\n",
    "# logp_unvec, logp_vec, score_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
